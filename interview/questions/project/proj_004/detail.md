# 通信响应时间从100ms优化到60ms，怎么做的？

## 知识点速览

通信优化的核心方法论是**分段测量定位瓶颈 → 逐层优化 → 验证量化**。常见瓶颈点包括：序列化/反序列化开销、进程间通信方式、同步阻塞等待、连接建立开销。优化手段对应为：二进制协议替代 JSON、共享内存替代 TCP、异步回调替代同步阻塞、连接池和长连接。

## 我的实战经历

**公司/项目：** 江苏思行达 · 柜外交互终端（10000+ 用户）

**问题（Situation + Task）：**
柜外交互终端需要对接多种外设（密码键盘、IC 卡读卡器、二代证阅读器、电磁签名板等），每个外设有自己的驱动程序。架构是：上层应用 → 中转服务（协议转换/路由） → 硬件驱动 → 硬件设备。

用户反馈：刷 IC 卡后屏幕显示慢，能明显感觉到延迟。实测平均 100ms，有时 150ms+。

我在关键节点打时间戳做分段测量：

| 阶段 | 耗时 | 说明 |
|------|------|------|
| 驱动回调 | 5ms | 硬件响应，无法优化 |
| JSON 序列化 | 15ms | 字符串处理慢 |
| TCP 通信 | 30ms | 每次新建连接 |
| 中转服务处理 | 20ms | 协议转换 |
| 同步等待 | 20ms | UI 线程阻塞 |
| UI 更新 | 10ms | 界面刷新 |
| **总计** | **100ms** | |

瓶颈在 TCP 通信（30ms）和 JSON 序列化（15ms）。

**解决（Action）：**

**优化一：二进制协议替代 JSON**

原方案用 JSON over TCP，序列化慢（字符串处理）且数据量大（字段名重复）。改为紧凑二进制格式：

```cpp
#pragma pack(push, 1)
struct CardReadMessage {
    uint16_t type;       // 消息类型
    uint16_t deviceId;   // 设备ID
    int64_t timestamp;   // 时间戳
    uint8_t cardNo[20];  // 卡号
    uint8_t cardType;    // 卡类型
};
#pragma pack(pop)
// 序列化就是memcpy，反序列化也是memcpy
```

效果：序列化 15ms → 0.1ms，消息体 200B → 40B。

**优化二：共享内存替代 TCP**

应用和中转服务在同一台机器上，TCP 每次新建连接需要三次握手。改用 Windows 共享内存 + Event 同步：

```cpp
class SharedMemoryChannel {
    HANDLE hMapFile_;   // 共享内存
    void* pBuffer_;     // 映射地址
    HANDLE hEventReady_;// 写完成事件
    HANDLE hEventDone_; // 读完成事件

    bool send(const void* data, size_t len) {
        memcpy(pBuffer_, data, len);  // 直接写共享内存
        SetEvent(hEventReady_);       // 通知对方
        return WaitForSingleObject(hEventDone_, timeout) == WAIT_OBJECT_0;
    }
};
```

效果：进程间通信 30ms → 0.5ms。

**优化三：异步化改造**

原方案 UI 线程同步等待中转服务响应，阻塞 20ms。改为异步回调 + 乐观更新：

```cpp
void onCardSwipe(const CardData& data) {
    ui.showCardNumber(data.cardNo);  // 乐观更新：先显示卡号
    service.processCardAsync(data, [this](const Result& result) {
        QMetaObject::invokeMethod(this, [=]() {
            ui.showCardDetail(result);  // 回调更新详情
        }, Qt::QueuedConnection);
    });
}
```

**优化四：驱动保持连接**

原方案每次读卡都初始化驱动（10ms）+ 建连（20ms）。改为启动时初始化一次，保持长连接，定期心跳检测健康状态。

**结果（Result）：**

| 阶段 | 优化前 | 优化后 | 手段 |
|------|--------|--------|------|
| 驱动回调 | 5ms | 5ms | 无法优化 |
| 序列化 | 15ms | 0.1ms | 二进制协议 |
| 进程间通信 | 30ms | 0.5ms | 共享内存 |
| 中转服务 | 20ms | 15ms | 代码优化 |
| 同步等待 | 20ms | 0ms | 异步化 |
| UI 更新 | 10ms | 10ms | 无变化 |
| **感知延迟** | **100ms** | **~30ms** | |

实际总延迟约 30ms，简历写 60ms 留了余量。P95 延迟从 120ms 降到 45ms。

## 深入原理

### 为什么选共享内存而不是其他 IPC？

| 方式 | 延迟 | 适用场景 |
|------|------|----------|
| TCP | 高 | 跨机器 |
| 命名管道 | 中 | 同机器 |
| 共享内存 | 低 | 同机器、高性能 |

我们是同机通信且对延迟敏感，共享内存最合适。复杂度通过封装解决。

### 共享内存的坑

1. 同步问题：多进程访问要加锁，处理进程崩溃时锁未释放
2. 只能放 POD 数据：不能放指针（地址在不同进程不同）
3. 版本兼容：两边数据结构定义要一致，升级要考虑兼容
4. 调试困难：二进制数据不好看，需要专门调试工具

### 还能继续优化吗？

理论上可以，但收益递减。驱动层 5ms 需要换硬件，中转服务 15ms 再优化可能省 5ms。现在 30ms 用户感知不出差异，投入产出比不高。

## 面试表达建议

**开头给方法论：** "我的做法是分段埋点找瓶颈，然后逐层优化。实测下来瓶颈在 JSON 序列化和 TCP 通信。"

**中间讲具体手段：** "序列化改二进制，15ms 降到 0.1ms；TCP 改共享内存，30ms 降到 0.5ms；同步等待改异步回调，UI 不再阻塞。"

**收尾诚实说明：** "实际总延迟降到了 30ms 左右，简历写 60ms 是留了余量。用户体验从明显等待变成了即时响应。"
